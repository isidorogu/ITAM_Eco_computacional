---
title: "Predicción de Abandono"
author: "Isidoro Garcia"
date: "2021"
output: pdf_document
urlcolor: blue
graphics: yes
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE,
                      fig.width = 7, fig.height = 4, fig.align = "right")
```

```{r, warning=FALSE}
library(tidyverse)
library(data.table)
library(broom)
library(knitr)
library(lubridate)
library(RCT)
library(gamlr)
library(ranger)
library(tree)
library(parallel)
library(tidymodels)
library(xgboost)
```



## Contexto

Cell2Cell es una compañía de teléfonos celulares que intenta mitigar el abandono de sus usuarios. Te contratan para 1) Encontrar un modelo que prediga el abandono con acierto y para usar los insights de este modelo para proponer una estrategia de manejo de abandono.


Las preguntas que contestaremos son:

1. Se puede predecir el abandono con los datos que nos compartieron? 

2. Cuáles son las variables que explican en mayor medida el abandono? 

3. Qué incentivos da Cell2Cell a sus usarios para prevenir el abandono?

4. Cuál es el valor de una estrategia de prevención de abandono focalizada y cómo difiere entre los segmentos de los usuarios? Qué usuarios deberían de recibir incentivos de prevención? Qué montos de incentivos

Nota: Voy a evaluar las tareas con base en la respuesta a cada pregunta. Como hay algunas preguntas que no tienen una respuesta clara, al final ponderaré de acuerdo al poder predictivo de su modelo vs las respuestas sugeridas. 



\newpage

## Datos

Los dotos los pueden encontrar en `Cell2Cell.Rdata`. En el archivo `Cell2Cell-Database-Documentation.xlsx` pueden encontrar documentación de la base de datos. 

Cargemos los datos
```{r }
load('Bases input/Cell2Cell.Rdata')

```

### 1. Qué variables tienen missing values? Toma alguna decisión con los missing values. Justifica tu respuesta

```{r q1 }
missings<-map_dbl(cell2cell %>% select_all(), 
                  ~100*sum(is.na(.)/nrow(cell2cell)))

(missings<-missings[missings>0])


# Revenue, mou, rcchrge, directas, overage, roam, changem changer, phones, models w/zero
cell2cell <-
   cell2cell %>% 
   mutate(across(names(missings[1:10]), ~if_else(is.na(.), 0, as.double(.))))

# eqpdays, age1, age2, quitar
cell2cell<-
   cell2cell %>% 
   filter(!is.na(eqpdays), !is.na(age1), !is.na(age2))
```

### 2. Tabula la distribución de la variable `churn`. Muestra la frecuencia absoluta y relativa. Crees que se debe hacer oversampling/undersamping?  

```{r q2}
kable(cell2cell %>% 
         group_by(churn) %>% 
         summarise(n = n()) %>%
         mutate(share = 100*n/sum(n)), digits = 2)
```


### 3. (2 pts) Divide tu base en entrenamiento y validación (80/20). Además, considera hacer oversampling (SMOTE) o undersampling. (Tip: Recuerda que el objetivo final es tener muestra ~balanceada en el traning set. En el validation la distribución debe ser la original)

La distribución esta 70% vs 30%. Si queremos construir un training set = 80%. Dentro del training, hay que hacer el undersampling tal que se balanceen las clases. 

```{r q3}

set_validation <-
   treatment_assign(data = cell2cell, 
                    share_control = 0.8, 
                    n_t = 1, strata_varlist = 'customer',
                    seed = 1908, key = 'customer')

set_validation<-set_validation$data
cell2cell<-
   left_join(cell2cell, set_validation %>% 
                ungroup %>% 
                select(-c(strata, missfit)))

# Divido entre training y validation
cell2cell_V<-
   cell2cell %>% 
   filter(treat == 1) %>% 
   select(-treat)

rm(set_validation)

cell2cell_T<-
   cell2cell %>% 
   filter(treat==0) %>% 
   select(-treat)

# Undersampling 
cell2cell_1_T<-
   cell2cell_T %>% 
   filter(churn==1)

undersampling<-
   treatment_assign(data = cell2cell_T %>% filter(churn==0), 
                    share_control = 0.41, 
                    n_t = 1, 
                    strata_varlist = 'customer', seed = 19987, key = 'customer')

undersampling<-undersampling$data

cell2cell_T<-left_join(cell2cell_T, 
                       undersampling %>% 
                          ungroup() %>% 
                          select(-strata, -missfit))

cell2cell_T<-bind_rows(cell2cell_T %>% filter(treat ==0), cell2cell_1_T)
table(cell2cell_T$churn)
rm(cell2cell_1_T, undersampling)

```


## Model estimation

Pondremos a competir 3 modelos: 

1. Cross-Validated LASSO-logit

2. Prune Trees

3. Random Forest

4.GBM

### 4 (2 pts). Estima un cross validated LASSO. Muestra el la gráfica de CV Binomial Deviance vs Complejidad

```{r q4}
# Matriz de covariates
cell2cell_T$treat<-NULL

X<-
   cell2cell_T %>% 
   ungroup() %>% 
   select(-customer, -churn)

X<-sparse.model.matrix(~.+0, data = X)
dim(X)
churn<-cell2cell_T$churn

# CV LASSO
detectCores()
cl<-makeCluster(12)

inicio<-Sys.time()
lasso<-cv.gamlr(x = X, y = churn, verb = T, cl = cl, family = 'binomial')
(tiempo<-Sys.time() - inicio)
stopCluster(cl)

save(lasso, file = 'Modelos/cv_lasso.Rdata')

# Plot del CV binomial dev
plot(lasso)

```
### 5. Grafica el Lasso de los coeficientes vs la complejidad del modelo.   

```{r q5}
plot(lasso$gamlr)

```

\newpage

### 6 (2 pts). Cuál es la $\lambda$ resultante? Genera una tabla con los coeficientes que selecciona el CV LASSO. Cuántas variables deja iguales a cero? Cuales son las 3 variables más importantes para predecir el abandono? Da una explicación intuitiva a la última pregunta

Las variables más relevantes son:

- `retcall` [El usuario ya recibió una llamada del equipo de retención]: Esto puede indicar que el usuario está próximo a terminar contrato o que las llamadas del equipo de retención en verdad tienen un efecto contraproducente en términos de la supervivencia del usuario. 

- `creditaa` [El usuario tiene la calificación crediticia más alta]: Esta disminuye la probabilidad de abandono. Muy útil en programas de pos-pago.

- `refurb` [El auricular del teléfono es reparado]: Esto podría indicar una experiencia negativa de uso de estos auriculares. 

```{r q6}
lasso$lambda.min

coeficientes<-as.matrix(coef(lasso, select = 'min'))

coeficientes<-tibble(variable = rownames(coeficientes),
                     coeficiente = coeficientes[,1])

coeficientes<-coeficientes %>% filter(coeficiente !=0)

kable(
   coeficientes<-
   coeficientes %>% 
   arrange(desc(abs(coeficiente))))
```

### 7. Genera un data frame (usando el validation set) que tenga: `customer`, `churn` y las predicciones del LASSO. 

```{r q7}
X<-
   cell2cell_V %>% 
   select(-customer, -churn)

X<-sparse.model.matrix(~.+0, data = X)

table(cell2cell_V$churn)
predicciones<-
   cell2cell_V %>% 
   select(customer, churn) %>% 
   mutate(lasso = drop(predict(lasso, newdata = X, select = 'min', type = 'response')))

```

### 8. Estima ahora tree. Usa `mindev = 0.05, mincut = 1000` Cuántos nodos terminales salen? Muestra el summary del árbol

```{r q8, cache=TRUE}
# Primero el árbol 
cell2cell_T<-
   cell2cell_T %>% 
   mutate(churn = factor(churn, levels = c("0","1")))


arbol<-tree(churn~., split = 'gini',
            cell2cell_T %>% ungroup() %>% select(-customer), 
            mindev = 0.05, mincut = 1000)

summary(arbol)

```

### 9. Grafica el árbol resultante 

```{r q9}
plot(arbol); text(arbol, pretty = 0)

```

### 10. Poda el árbol usando CV. Muestra el resultado. Grafica Tree Size vs Binomial Deviance. Cuál es el mejor tamaño del árbol? Mejora el Error?

```{r q10}
(cv_arbol<-cv.tree(arbol, K = 5))

arbol_graf<-
   tibble(size = cv_arbol$size, binomial_deviance = cv_arbol$dev)


ggplot(arbol_graf, aes(size, binomial_deviance))+geom_point()+geom_path()+
   theme_bw()

```

### 11. Gráfica el árbol final. (Tip: Checa `prune.tree`)

```{r q11 }
arbol_prune<-prune.tree(arbol, best = 4)

plot(arbol_prune); text(arbol_prune, pretty = 0)

save(arbol_prune, file = 'Modelos/arbol.Rdata')
```

### 12. Genera las predicciones del árbol pruned. Guardalas en la base de predicciones. Guarda el score y la prediccion categorica en la misma data frame donde guardaste las predicciones del LASSO

```{r q12 }
X<-
   cell2cell_V %>% 
   select(-customer, -churn)

predicciones<-
   predicciones %>% 
   mutate(arbol_class = drop(predict(arbol_prune, newdata = X, type = 'class')), 
          arbol_prob = drop(predict(arbol_prune, newdata = X, type = 'vector'))[,2])

``` 

### 13 (4pts). Corre un Random Forest ahora. Cuál es la $B$ para la que ya no ganamos mucho más en poder predictivo?

- Corre para `num.trees=100,200,300, 500, 700, 800`

- En cada caso, guarda únicamente el `prediction.error`

- Recuerda fijar el parámetro `probability-T` para que el RF genere predicciones categoricas y sus probabilidades


```{r q13, cache=TRUE }

X<-
   cell2cell_T %>% 
   ungroup() %>% 
   select(-customer, -churn)

X<-sparse.model.matrix(~.+0, data = X)
dim(X)
churn<-cell2cell_T$churn

# Grid de Random Forests
detectCores()
cl<-makeCluster(12)

inicio<-Sys.time()
random_forest<-map(c(100,200,300,500, 700, 800), 
                   function(z) 
                      ranger(x = X, 
                             y = churn, 
                             importance = 'impurity', 
                             probability = T,
                             num.trees = z))

(tiempo<-Sys.time() - inicio)
stopCluster(cl)

error_prediccion<-tibble(trees = c(100,200,300,500, 700, 800), 
                         oob_error = map_dbl(random_forest, ~.$prediction.error))


ggplot(error_prediccion, aes(trees, 100*oob_error))+
   geom_point()+geom_path()+theme_bw()

random_forest<-random_forest[[6]]

save(random_forest, file = 'Modelos/random_forest.Rdata')
``` 




### 14. Escoge un random forest para hacer las predicciones. Grafica la importancia de las variables. Interpreta 

```{r q14}

variable_importance<-random_forest$variable.importance

variable_importance<-tibble(variable = names(variable_importance),
                            importance = variable_importance)

ggplot(variable_importance %>% filter(importance> 100), 
       aes(fct_reorder(variable, importance), importance, fill = variable))+
   geom_col()+coord_flip()+theme_bw()+theme(legend.position = 'none')+
   labs(x = 'Variable', y = 'Importancia')
```

### 15. Genera las predicciones OOS para el random forest. Guardalas en la misma data.frame que los otros modelos 

```{r q15}

# predicciones continuas
X<-
   cell2cell_V %>% 
   select(-customer, -churn)

X<-sparse.model.matrix(~.+0, data = X)



predicciones<-
   predicciones %>% 
   mutate(random_forest_prob = predict(random_forest, data = X)$predictions[,2])


rm(a, arbol, arbol_graf, arbol2, cl)

save(predicciones, file = 'Bases output/predicciones.Rdata')
```

### 16. Estima un GBM. 

- Encuenta el número de boosting rounds ideal $B$

- Encuentra la profundidad de los árboles $d$

- Estima un grid de modelos para calibrar los dos hiperparametros

```{r q16, cache=TRUE}
library(tidymodels) 
library(xgboost)


X<-
   cell2cell_T %>% 
   select(-customer, -churn)

X<-sparse.model.matrix(~.+0, data = X)

churn<-as.numeric(cell2cell_T$churn)-1


# Creating the grid that covers must of the hyper space 
xgb_grid<-grid_latin_hypercube(
                tree_depth(), 
                loss_reduction(), 
                sample_size = sample_prop(), 
                learn_rate(),
                mtry(c(1,ncol(X))),
                size = 15)



# XGBOOST matrix 
training<-xgb.DMatrix(data = X, label = churn)


# Validation
X<-
   cell2cell_V %>% 
   select(-customer, -churn)

X<-sparse.model.matrix(~.+0, data = X)

churn<-cell2cell_V$churn

validation<-xgb.DMatrix(data = X, label = churn)

# Watchlist 
watchlist<-list(training = training, validation = validation)
# names(watchlist$validation)<- names(watchlist$training)

# transforming xgb_grid to have proper names 
xgb_grid<-xgb_grid %>% 
                rename(max_depth = tree_depth, 
                       gamma = loss_reduction, 
                       subsample = sample_size, 
                       eta = learn_rate, 
                       colsample_bytree = mtry) %>% 
                mutate(eval_metric = 'auc', 
                       colsample_bytree = colsample_bytree/66) 


# Modelo
listas<-map(1:15, ~as.list(xgb_grid[.,]))

a<-Sys.time()
xgb<-map(listas, 
             function(x) {
               xgb.train(params = x, 
                         data = training, 
                         nrounds = 300, 
                         objective = "binary:logistic", 
                         early_stopping_rounds = 5, 
                         verbose = 1, 
                         watchlist = watchlist) })
Sys.time() -a
save(xgb, xgb_grid, training, watchlist, file = "Modelos/xgb.Rdata")



# Winning combination
params_xgb<-map_dfr(xgb, ~.$params)
params_xgb<-bind_cols(params_xgb, auc = map_dbl(xgb, function(x) x$best_score))  
params_xgb$model_number<-seq(1:15)

# tabla 
kable(params_xgb)

# Quedandome con el ganador 
a<-pull(params_xgb %>% arrange(desc(auc)) %>% dplyr::slice(1) %>% select(model_number))

xgb<-xgb[[1]]

# Predicting 
# Validation
X<-
   cell2cell_V %>% 
   select(-customer, -churn)

X<-sparse.model.matrix(~.+0, data = X)

predicciones<-
  predicciones %>% 
  mutate(xgb = predict(xgb, newdata = X))

save(predicciones, file = 'Bases output/predicciones.Rdata')

```

### 17 (4 pts). Genera graficas de las curvas ROC para los 4 modelos. Cual parece ser mejor?

```{r q17}
# Factor truth
predicciones<-
   predicciones %>% 
   mutate(churn = factor(churn, levels = c('1', '0')))

# ROC para cada una
predicciones1<-
   predicciones %>% 
   pivot_longer(cols = c("lasso", "arbol_prob", "random_forest_prob", "xgb"), 
                names_to = 'modelo', 
                values_to = 'pred')

bases<-predicciones1 %>% split(.$modelo)

rocs<-map2_dfr(.x =  bases, 
               .y = names(bases), 
               function(x,y) roc_curve(data = x, churn, pred) %>% 
                  mutate(modelo = y)) 


ggplot(rocs, aes(1-specificity, y = sensitivity))+
   geom_path()+
   geom_abline(lty =3)+coord_equal()+theme_bw()+
   facet_wrap(~modelo)
```

### 18. Genera una tabla con el AUC ROC. Cuál es el mejor modelo ? 

El random forest marginalmente sobre el XGB

```{r q18}
auc_rocs<-map2_dfr(.x =  bases, 
               .y = names(bases), 
               function(x,y) roc_auc(data = x, churn, pred) %>% 
                  mutate(modelo = y)) 

kable(auc_rocs)

```

### 19 (2 pts). Genera las gráficas de las curvas precission recall para los 4 modelos. Cuál parece ser mejor ahora?

Ahora el XGB parece ser mejor que el random forest


```{r q19}
pr_curves<-map2_dfr(.x =  bases, 
               .y = names(bases), 
               function(x,y) pr_curve(data = x, churn, pred) %>% 
                  mutate(modelo = y)) 

save(rocs, pr_curves, file = 'Tablas/rocs_pr.Rdata')

ggplot(pr_curves, aes(recall, y = precision))+
   geom_path()+
   coord_equal()+theme_bw()+
   facet_wrap(~modelo)


# PR AUC 
auc_pr<-map2_dfr(.x =  bases, 
               .y = names(bases), 
               function(x,y) pr_auc(data = x, churn, pred) %>% 
                  mutate(modelo = y)) 

kable(auc_pr)


```

### 20 (2pts). Escoge un punto de corte para generar predicciones categoricas para el LASSO basado en la Curva ROC. Genera las matrices de confusión para cada modelo. Compáralas. Qué tipo de error es mas pernicioso? 

```{r q20}
# Cortes usando ROC 
cortes<-
   rocs %>% 
   group_by(modelo) %>% 
   filter(sensitivity >= 0.80) %>% 
   arrange(desc(specificity)) %>% 
   dplyr::slice(1) 
cortes_pr<-
  pr_curves %>%
  group_by(modelo) %>% 
  filter(recall>=0.8) %>%
  arrange(desc(precision)) %>% 
  dplyr::slice(1) %>% 
  select(modelo, everything())


predicciones<-
   predicciones %>% 
   mutate(lasso_cat = if_else(lasso>=cortes_pr$.threshold[cortes_pr$modelo=='lasso'], 1, 0), 
          rf_cat = if_else(random_forest_prob>=cortes_pr$.threshold[cortes_pr$modelo=='random_forest_prob'], 1, 0),
          xgb_cat = if_else(xgb>=cortes_pr$.threshold[cortes_pr$modelo=='xgb'], 1, 0))


# Matrices 
kable(prop.table(table(predicciones$lasso_cat, as.numeric(as.character(predicciones$churn))), margin = 2), digits = 2, caption = 'Matriz de Confusión LASSO')

kable(prop.table(table(predicciones$arbol_class, as.numeric(as.character(predicciones$churn))), margin = 2), digits = 2, caption = 'Matriz de Confusión Prune Tree')

kable(prop.table(table(predicciones$rf_cat, as.numeric(as.character(predicciones$churn))), margin = 2), digits = 2, caption = 'Matriz de Confusión RF')

kable(prop.table(table(predicciones$xgb_cat, as.numeric(as.character(predicciones$churn))), margin = 2), digits = 2, caption = 'Matriz de Confusión XGB')
```

### 21 (4pts). Construye una lift table. Esto es, para 20 grupos del score predecido de manera decreciente, genera 1) El promedio de las predicciones, 2) el promedio del churn rate observado. Estima el lift: 

$$lift_k = \frac{churn \ \ rate_k}{churn \ \ rate \ \ promedio}$$

Donde churn rate promedio es el promedio global. Muesta la tabla. Que te dice el lift intuitivamente

Intuitivamente, el lift me dice si el nivel de churn observado es más alto en scores más altos o no. Esperarías ver una relación monotónica entre el lift y el score promedio 

```{r q21}
# ROC para cada una
predicciones1<-
   predicciones %>% 
   pivot_longer(cols = c("lasso", "arbol_prob", "random_forest_prob", "xgb"), 
                names_to = 'modelo', 
                values_to = 'pred')



predicciones1<-
   predicciones1 %>% 
   group_by(modelo) %>% 
   mutate(score_group = ntile(x = pred, 20)) %>% 
   group_by(modelo, score_group) %>% 
   summarise(pred  = mean(pred), 
             churn = mean(as.numeric(as.character(churn))))


base_grafica<-
   predicciones1 %>% 
   pivot_longer(cols = c(pred, churn), names_to = 'tipo', values_to = 'value')

ggplot(base_grafica, aes(score_group, value, fill = tipo, color = tipo))+
   geom_point()+geom_line()+
   facet_wrap(~modelo)+
   theme_bw()+
   labs(title = 'Relación Score realidad')+
   theme(legend.position = 'bottom')

```

### 22 (4pts). Construye un Gain table. Esto es: Cuántas personas detectarías con el modelo del total de churned users al buscar al x% de la población? Como se compara esto para una selección aleatoria? 

Ejemplo: Si tengo 10% de churn y tengo 10 grupos:

- Si focalizo al 10% aleatoriamente, voy a obtener el 10% de la población churned. 

- Si focalizo al 20% aleatoriamente, voy a obtener el 20% de la población churned. 

- Si focalizo al 10% usando al 10% de la población con score predecido mas alto, que porcentaje obtengo? 

- Como se compara tu modelo vs el modelo aleatorio

```{r q22}
# Gain 
predicciones1<-
   predicciones %>% 
   pivot_longer(cols = c("lasso", "arbol_prob", "random_forest_prob", "xgb"), 
                names_to = 'modelo', 
                values_to = 'pred')


gain<-
   predicciones1 %>% 
   group_by(modelo) %>% 
   mutate(score_group = ntile(x = pred, 10),
          total_churn = sum(as.numeric(as.character(churn)))) %>% 
  ungroup() %>%
    group_by(modelo, score_group) %>% 
   summarise(gain  = sum(as.numeric(as.character(churn)))) %>% 
  group_by(modelo) %>% 
  mutate(total_churn = sum(gain), 
         score_group = abs(score_group-10)+1, 
         gain = gain/total_churn) %>% 
  group_by(modelo) %>% 
  arrange(score_group) %>%
  mutate(cum_gain = cumsum(gain))



# Graficas 
ggplot(gain, aes(score_group, cum_gain))+geom_point()+geom_line()+
  geom_abline(intercept = 0 ,slope = 1/10)+facet_wrap(~modelo)+
  theme_bw()

```

### 23 (2pts). Calcula el AUC Gain del mejor modelo. Interpreta

```{r q23}
library(pracma)

bases <- gain %>% split(.$modelo)

map(bases, ~trapz(.$score_group, .$cum_gain)/10)

```

### 24. Concluye. Que estrategia harías con este modelo? Cómo generarías valor a partir de el?

Haría una estrategia focalizada de prevencion de churn con el modelo usando el Random Forest. Se que estaré atacando al 80% de los casos (recall) y que la gente que ataque tendré una precisión de ~34%. Al ser una estrategia que no genera dolor, no hay problema con tener baja precisión. Así mismo, puedo ver que con sólo tratar al 30% de la base, puedo capturar al ~50% de la base de churn users!!! 


