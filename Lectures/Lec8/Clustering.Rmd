---
title: 'Lec8: Unsupervised Learning I'
author: "Isidoro Garcia Urquieta"
date: "2023"
output: beamer_presentation
slide_level: 3
fontsize: 10pt
header-includes: 
   \usepackage{graphicx}
   \usepackage{fancyhdr}
   \pagestyle{fancy}
   \setlength\headheight{3pt}
   \fancyhead[R]{\includegraphics[width=2cm]{logo-ITAM.png}}
   \lhead{\fontsize{8pt}{10pt}\selectfont \textit{Economía Computacional}}
   \definecolor{verdeitam}{RGB}{0,90,0}
   \setbeamercolor{structure}{fg=verdeitam}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

### Agenda

- Aprendizaje No Supervisado 

- Clustering

- K-means 

- Hierarchical Clustering


### Aprendizaje Estadístico 

El aprendizaje estadístico concierne dos tipos: 

1) **Aprendizaje Supervisado**: Existe una variable endógena que gobierna el aprendizaje. Esto genera la siguiente función a estimar: 

$$Y = f(X) + \epsilon$$

Donde $X: X_1,...,X_p$ son todas las variables explicativas que se nos ocurran, $Y$ es la variable supervisora y $\epsilon$ es un error aleatorio. 

2) **Aprendizaje No Supervisado**: No existe una variable objectivo. Tenemos $X$ y queremos entender como se relacionan entre ellas. Típicamente relacionado a método de reducción de dimensionalidad o segmentación. 

### Aprendizaje No supervisado

Existen dos tipos de objetivos en el aprendizaje no supervisado: 

1. **Clustering**: Se trata de segmentar a las observaciones en grupos (clusteres) tal que los observaciones dentro de cada grupo sean muy similares y disimilares a observaciones en otros grupos. 

2. **Factor Models**: Se trata de *reducir dimensionalidad* a partir de una $X$ para entender los factores principales. 

En ambos, estamos buscando simplificar cómo vemos el conjunto de las $X_s$. A pesar de que no buscamos predicción, buscamos modelos *estables* a nuevas observaciones. 


### Clustering 

Por qué hacer clustering? 

- Un resumen: Mostrar representaciones de los datos interesantes que reflejen la varianza de toda la base. 

- Descubrimiento: Encontrar sub grupos interesantes dentro de todo el set de observaciones. 


En las técnicas de clustering, buscamos encontrar las membresías a unos subgrupos **desconocidos** a partir de algoritmos. 

Noten como esto hace que el unsupervised learning sea un reto y con algunos toques más artísticos. La razón es porque no tenemos una $y$ que corrobore que hicimos un buen o mal trabajo. 

### Clustering Ejemplos prácticos

- Un buscador o plataforma quiere encontrar grupos de  productos similares (Para ofrecerlos a los usuarios): Netflix haciendo recomendaciones de películas similares a las que ya viste. 

- Sentimiento de las personas en Twitter u otra red social: Tenemos una base de $X$ de palabras, tweets y queremos resumir esta información en un score de sentimiento. 

- Encontrar los temas de los que habla un documento de manera automatizada. 


- User personas (Arquetipos): Encontrar grupos de usuarios similares que ayuden a generar entendimiento más profundo de los usuarios. 


### Medidas de disimilarity

Como el clustering busca grupos de observaciones que sean muy similares dentro del grupo y muy disimilares a otros grupos, tenemos que definir las distancias! 

Cómo medimos la distancia entre dos observaciones? Imaginen tenemos filas $i \in \{1,N \}$ y variables $X_p$, $p \in \{1, P\}$. 


La distancia entre dos observaciones $i$ y $j$ es:

Para variables continuas: Distancia euclidiana 

$$d(i,j)= [\Sigma_{p=1}^P(x_{ip} - x_{jp})^2]^{0.5}$$
Noten dos cosas:

- Estamos midiendo la dsitancia entre dos observaciones (filas!)

- Lo hacemos para cada columa $p$ y luego agregamos la distancia entre todas las columnas!


### Matriz de Disimilarity $D$

Que resulta de aplicar la función de distancia entre cada punto $i$ vs todos los otros? Un Matriz! 

Características de la matriz *$D$*:

- Tiene dimensiones $NxN$: Es decir, N filas y N columnas

- Es simétrica! 

- La diagonal es cero: La distancia entre cualquier punto y sí mismo es cero


### Ejemplo 

```{r, error=FALSE, warning=F, message=F, echo=T}
library(tidyverse)
set.seed(1990)
data<-tibble(x=rnorm(n = 5, 5, 2),
             y=runif(5, 0, 100),
             z=rnorm(5, 15, 3))


(matriz_D<-dist(data, diag = T))

```

### Ejemplo 

```{r, error=FALSE, warning=F, message=F, echo=T}
data<-
  data %>% 
  mutate_all(function(x) (x - mean(x))/sd(x))

(matriz_D<-dist(data, diag = T))
```



### Tipos de Clustering 

1. **Métodos basados en modelos mixture (Mixture models)**: Se asume que cada $x_{ip}$ viene de 1 de los K mixture components. Donde definimos la probabilidad de pertenecer a este grupo como $p_k(x)$ para $k \in \{1,..,K \}$. 

2. **Métodos basados en heurísticas**: Son modelos agnosticos a K, se construyen modelos jerárquicos que dividan mejor a las observaciones. 
Empecemos con 1) K-means 

### K-means 

K-mean es un método de clusterización no paramétrico que, dandole el número de grupos $K$ deseado, asigna a cada observación a **un** grupo en particular. El algoritmo crea grupos mutuamente excluyentes: 

1. $C_1 \cup \ \ C_2  \ \ \cup ...\cup \ \ C_k=\{1,...,n\}$

2. $C_k \cap \ \ C_{k'} = \emptyset$ para $C_k \neq C_{k'}$


Por ejemplo, si la observación i cae en el cluster k, decimos $i \in C_k$. La idea de Kmeans es encontrar clusters en las que las observaciones **dentro** del cluster sean muy parecidas. Esto es, la variación intra-cluster sea lo más baja posible.

Que creen que va a intentar minimizar???


### K-means 

Formalmente, buscamos minimizar el *within* variation:

$$min_{\{C_1, C_2, ..., C_k \}}\Sigma_{k=1}^KW(C_k)$$

Intuitivamente, queremos clusterizar a las observaciones en k grupos tal que la *suma* de las variaciones intra-cluster sean lo más bajo posible. 

Si usamos $W(C_k) = d(i,j)$ Tenemos que $W(C_k)=\frac{1}{N_k}\Sigma_{i \in C_k}\Sigma_{p=1}^P(x_{ip} - x_{jp})^2$



Tenemos encontrar que minimiza la distancia entre las filas dentro de cada cluster, para todas las variables p y para todos los clusters k. 

### K-means algoritmo 

Pasos: 

Escoge un número $K$

1. Asigna aleatoriamente cada observación a un cluster $C_k, \ \ k \in \{1.K\}$

2. Calcula los centroides $\hat\mu_k$ para las $p$ variables en la base

$$\mu_p = \frac{1}{N_k}\Sigma_i^{N_k}x_{ip}$$

3. Asigna a cada observación al centroide al que este más cercano. 

4. Repite hasta que la asignación de cada observación deje de cambiar. 


Problema: La solución depende de la iniciación aleatoria. Es decir, el algoritmo encuentra un óptimo local en lugar de un óptimo global. Por ello, es mejor estimar K-means varias veces con inicios aleatorios distintos. 


### Ejemplo 

\begin{center}
\includegraphics[width = 8cm]{algorithm.png}
\end{center}



### Ejemplo 

\begin{center}
\includegraphics[width = 11cm]{k_means.png}
\end{center}

### Ejemplo II

Acá vemos para distintos intentos. El segundo da mejor separación.

\begin{center}
\includegraphics[width = 10cm]{k_means2.png}
\end{center}

### K-means en `R` 

La función `kmeans(x,centers,nstart)` se utiliza para estimar K-means. Donde `x` es una `data.frame` **númerico**, `centers` = $K$ y `nstart` es el número de inicios aleatorios. 


\begin{center}
\includegraphics[width = 10cm]{kmeans_R.png}
\end{center}

`grp$cluster` guarda la pertenencia de cada observación a los clusteres $k$

### K-means en `R`

\begin{center}
\includegraphics[width = 11cm]{kmeans_R2.png}
\end{center}


### Cómo escoger $K$

La selección de K es altamente subjetiva. Por ello es importante seguir algo de intuición en la decisión de $K$.

No obstante, podemos hacer un algoritmo parecido a los modelos predictivos: 

1. Elige un set de K candidatas $K_1<K_2<...<K_m$

2. Utiliza AICc/BIC para decidir el mejor approach. 

En el caso de K-means. Los parámetros estimados son $K \times p$. Dado que el BIC no asume distribuciones, es normalmente preferido. No obstante, no son una regla fija para elegir $K$.




### Problemas con K-means 

Hay dos problemas muy reconocidos sobre K-means

- Como iniciamos asignando las observaciones de manera aleatoria, K-means puede encontrar un mínimo local, no global

- La media de los variables (centroides) es muy sensible a outliers. Esto puede hacer que no se llegue a la segmentación adecuada 


### K-mediods 

El algoritmo de K-mediods busca solucionar este problema mediante un cambio: En lugar de enfocarse en los centroides, se enfoca en la **observación central** de cada cluster 

Escoge un número $K$

1. Asigna aleatoriamente cada observación a un cluster $C_k, \ \ k \in \{1.K\}$

2. Calcula la centroides de cada $C_k$ como la observación que tiene una distancia total más pequeña:

$$i_k*= argmin_{\{i: C(i)=k \}} \Sigma_{i \in C_k}D(x_i,x_j) $$


3. Asigna a cada observación al centroide al que este más cercano. 

4. Repite hasta que la asignación de cada observación deje de cambiar. 


### K-mediods vs K-means

Pros K-means:

- Es mucho más rápido calcular medias que distancias 


Pros K-mediods:

- Es más robusto a outliers

Alternativa: K-medianas. Rápido y robusto


### K-mediods en `R`

```{r, echo=T }
library(cluster)
x <- rbind(cbind(rnorm(10,0,0.5), rnorm(10,0,0.5)),
           cbind(rnorm(15,5,0.5), rnorm(15,5,0.5)))
pamx <- pam(x, 2)
pamx # Medoids: '7' and '25' ...
summary(pamx)



```

### K-mediods

```{r}
clusplot(pamx)

```



### Hierarchical Clustering

Una ventaja y desventaja de K-means es que estima **exactamente** los K grupos que le pedimos. 

  - Si elegimos bien $K$ o es lo que necesita el caso de negocio/policy, es ventaja. 
  
  - Si no tenemos un prior de $K$ puede ser problemático.
  
**Hierarchical Clustering** es una alternativa que no requiere un modelado previo ni una $K$. 

- Produce tree-based representaciones de los datos. 

- Esto genera clusteres anidados (grandes clusteres, pequeños clusteres dentro de esos)

- Observaciones similares igualmente terminarán en los mismos clusteres. 

### Hierarchical Clustering

Algo muy importante es que Hierarchical Clustering es un método **aglomerativo** (bottom-up).

- Aglomerativo: 
 
 - Todas las observaciones empiezan siendo un cluster (tienes $n$ clusteres).
 
 - Fusiones a los dos grupos más similares de manera iterativa hasta que terminas con un sólo grupo.
 
- Divisivo (top-down):

 - Empiezas con un cluster para $n$ observaciones
 
 - Separas en dos grupos maximizando la inter-varianza y minimizando la intravarianza (Ojo! aquí no tenemos $y$ para eso). 
 
Los métodos aglomerativos son más sencillos. Empecemos ahí. 

### Hierarchical Clustering

\begin{center}
\includegraphics[width = 11cm]{aglomerativo.png}
\end{center}

### Hierarchical Clustering 

Esto se puede representar como un árbol constuído de abajo hacía arriba: 

\begin{center}
\includegraphics[width = 11cm]{dendogram.png}
\end{center}

### Donde cortamos el árbol

Noten como cada nodo terminal de este dendograma es una observación. Por ende, tenemos que decidir en donde cortarlo para encontrar los números de clusteres $K$. Una ventaja de esto es que una corrida te puede  
dar todas las opciones de K posibles. 

\begin{center}
\includegraphics[width = 10cm]{dendogram_cut.png}
\end{center}

### Algoritmo Hierarchical Clustering

Formalicemos el algoritmo. Pasos:

1. Escoges una medida de similitud (i.e. Distancia Euclideana). 

2. Empiezas con todas las observaciones en su cluster ($n$ clusteres).

3. Calculas $n \choose 2$ métricas de distancia. 

4. Encuentra el par más similar y fusionalo en un cluster. La métrica de similitud es la altura de corte del dendograma

5. Repite 3 y 4 con los $i-1$ clusteres restantes. 

Retos: Cómo defines la distancia entre 2 clusteres cuándo uno o ambos tienen más de una observación? Definimos *linkage*

### Linkage

\begin{center}
\includegraphics[width = 10cm]{linkage.png}
\end{center}

En general $Average = Complete > Single>Centroid$

### Linkage: Impacto en el dendograma

\begin{center}
\includegraphics[width = 11cm]{linkage2.png}
\end{center}

### Hierarchical Cluster en `R`

La función `hclust` genera Hierarchical Clustering. `dist(X)` para distancia euclideana. Despues se utiliza `cuttree(h,x)` para cortar al nivel del dendograma. 

\begin{center}
\includegraphics[width = 9cm]{hierarchical.png}
\end{center}



