---
title: 'Lec7: Random Forest y XGB'
author: "Isidoro Garcia Urquieta"
date: "2023"
output: beamer_presentation
slide_level: 3
fontsize: 10pt
header-includes: 
   \usepackage{graphicx}
   \usepackage{fancyhdr}
   \pagestyle{fancy}
   \setlength\headheight{3pt}
   \fancyhead[R]{\includegraphics[width=2cm]{logo-ITAM.png}}
   \lhead{\fontsize{8pt}{10pt}\selectfont \textit{Economía Computacional}}
   \definecolor{verdeitam}{RGB}{0,90,0}
   \setbeamercolor{structure}{fg=verdeitam}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

### Agenda

- Mini Repaso Trees 

- Bootrapping

- Bagging

- Random Forests 

- Boosting 

- Gradient Boosting Machines




### Resumen Trees

- Los árboles de decisión (Regresión y Clasificación) son algoritmos no paramétricos

- Son fáciles de interpretar! 

- Detectan no linealidades de manera automática

- Se estiman de manera **greedy**, donde se buscan splits que maximicen la varianza entre nodos (inter-varianza) y se minimice la varianze dentro de cada nodo (intra-varianza)

- Hay dos maneras de parar la estimación de los arboles: observaciones mínimas en los nodos terminales o disminución en el deviance mínima. 

- El greediness es miope. Esto es, un split malo puede seguirle un gran split. 

- Es díficil evitar el overfitting en los árboles. 

### Cost complexity pruning 

Cost complexity pruning (Weakest link pruning) es la manera más común de reducir el número de modelos candidatos y aplicar k-fold CV. 

El proceso funciona así: 

1. Estima árbol de manera greedy hasta llegar a `min obs`. 

2. Aplica un parámetro de complejidad $\alpha$ que castigue la complejidad del árbol. Esto te dará una secuencia de subárboles como función de $\alpha$.

3. Aplica k-fold CV para escoger $\alpha$ 

### Cost complexity pruning 


Veamos la formula de coxt complexity pruning:

$$\Sigma_{j=1}^J\Sigma_{R_j}(y_i - \hat y_i)^2 + \alpha|T|$$
Donde $T$ es el número de nodos terminales. Se ve familiar? 

Resulta que si un $\alpha$ muy grande, tendremos un árbol con $T_0 \subset T$. Mientras $\alpha$ tiende a cero $T_0 \rightarrow T$. 

$\alpha$ nos ayuda a tener un set de candidatos de modelo estimable vía K-fold CV!!


### Grafica de desempeño de CV tree

Noten como el CV es un buen estimador del desempeño fuera de la muestra (3 nodos terminales). Adicionalmente, noten como en el training set, el árbol siempre parece que va a estimar mejor mientras más grande haga los árboles.

\begin{center}
\includegraphics[width = 8cm]{cv_tree.png}
\end{center}


### Resumen Tree

- Para ayudar al overfitting, podemos hacer Tree pruning con Cross Validation 

- Esto hara que estimemos greedy y luego tengamos un set de árboles candidatos dependiendo de cuánto castigamos la complejidad de los mismos.

- Los prune.trees son más poderosos (predicción) que los árboles normales. Por ende, preferibles. 

- Aún así, los árboles son estimadores de alta varianza. Esto es, varían mucho dependiendo de la muestra en la que nos encontremos (no paramétrico). Por ende, necesitamos alguna manera de estabilizar las predicciones de los árboles para lograr mejor poder predictivo OOS. 


### Boostrapping 

Llegamos al segundo método de **remuestreo** (El primero fue CV)! 

El boostrapping es muy útil para conseguir errores estandares e intervalos de confianza cuando la distribución de algún estimador es desconocida, tenemos pocas observaciones o es muy díficil de estimar. 

Por ejemplo, en el caso de la econometría, podemos inferir la distribución de nuestro estimador $\hat \beta$ al remuestrear la base **con reemplazo** y estimar $\beta$ con cada muestra.  

Lo bonito del Boostrapping es que tiene otras aplicaciones además de la generación de errores estandár. Se puede aplicar a mejorar el poder predictivo de nuestros modelos estadísticos. Podemos usarlo para disminuir la varianza de los árboles.

### Boostrapping 

Pasos: 

Dada una base de datos $\{ z_i\}_{i=1}^n$, para $b \in \{1,2,3...,B \}$ muestras: 

1. Tomas una muestra **con reemplazo** $\{ z_i^b\}_{i=1}^n$

2. Estimas $\beta_b$ usando la base de (1)

Luego, $\{ \beta_b\}_{b=1}^B$ es una aproximación de la distribución de $\hat\beta$

El vector de tamaño $B$ $\{ \beta_b\}_{b=1}^B$ puede ser usado como la distribución teorica que aprendimos en la clase 2 (inferencia!) que usaba el Teorema del Límite Central. 

$$\beta \in \hat \beta \pm 2sd(\hat \beta_b) $$
$$se(\hat \beta) \simeq sd(\beta_b)=\sqrt{\frac{1}{B}\Sigma_b(\hat \beta_b - \hat \beta)}$$
Donde $\hat \beta$ es el estimador original usando toda la base de datos. 

### Boostrapping 

Como podríamos usar Bootstrapping para mejorar un modelo predictivo usando boostrapping? Cómo se combinaría con Cross-Validation? 





### Bagging 

Los árboles de decisión sufren alta varianza. Esto es, cuando dividimos la muestra en entrenamiento y validación y entrenamos el árbol (c/ CV pruning), los resultados pueden ser muy distintos en cada base. 

En el caso del LASSO, esto no era así. Porqué? 

  - Por que teníamos **estimadores** paramétricos que se estabilizan con cierta facilidad ($n>p$). 
  
El **Boostrapping Aggregation (Bagging!)** es un procedimiento muy utilizado para estabilizar la varianza en aprendizaje estadístico. 


### Bagging 

El Bagging utiliza el poder de las medias para crear estimadores estables. 

Recuerden que si tenemos un set de $n$ observaciones independientes $Z_1,Z_2,...,Z_n$ y *tenemos un procedimiento insesgado*: 

Promediamos las $Z_i$: $\bar Z$

Sabemos que:

$$\bar{Z}_n \rightarrow^DN(\mu_Z,\sqrt{\sigma^2/n})$$

En otras palabras, la varianza original de los datos $\sigma^2$ se reduce cuando sacamos promedios a $\sigma^2/n$! 


### Bagging en Árboles de Decisión

Pasos para aplicar Bagging en árboles de decisión: 

1. Construye $B$ muestras con reemplazo

2. Construyes un árbol en cada muestra $B$: $\hat f^b(X)$ $b \in \{1,...,B\}$

3. Construye el Bag estimate: 

  - Para árboles de regresión: $\hat f_{bag}(X) = \frac{1}{B}\Sigma_{b=1}^B \hat f^b(X)$
  
  - Para árboles de clasificación: Majority vote. La clase más vota es a predicción.

Queda pendiente algo: Cuántas submuestras $B$ tomar? Con $B$ alto caemos en overfitting?

### Bagging

Cuantas $B$ tomar? 

- En la práctica, necesitamos $B$ suficientemente grandes para estabilizar la varianza. Esto implica prueba y error en la práctica.

- Otro punto importante es que un número grande $B$ no induce overfitting, al contrario. 

- Con: Estimar los árboles con demasiadas submuestras puede ser computacionalmente complejo. No obstante es alcanzable en una buena computadora (Parallel Computing!)

### Bagging 


\begin{center}

Cómo decidirían cuantos árboles estimar usando Bagging?

\end{center}

### Out-of-Bag

Resulta que, como en Cross-Validation, hay manera de estimar el error OOS desde la muestra de entrenamiento. 

Recuerden que en cada muestra $B$, dejamos algunas observaciones sin tomar. Estas observaciones pueden servir como estimador del error. Su nombre es out of bag. 

Los resultados muestran que en la práctica, los bagging trees superan en poder de predicción a los trees y pruned trees. 


Terminamos? que cosas se les ocurren que pueden seguir pasando con los Bagging Estimators?

### Interpretabilidad 

El bagging nos permite estabilizar las predicciones de los árboles y ganar poder predictivo. 

El costo asociado es la **interpretabilidad**

```{r echo=FALSE, fig.height=4.5, message=FALSE, warning=FALSE}
library(tidyverse)
options(scipen = 99)
grafica2<-
   tibble(flexibilidad = seq(1, 4, 1), 
          interpretabilidad = seq(4,1,-1), 
          metodo = c('LASSO', 'Regresión', 'Árboles de decisión', 'Bagging, Boosting'))

ggplot(grafica2, aes(flexibilidad, interpretabilidad, label = metodo))+geom_text()+
   theme_classic()+scale_y_continuous(breaks = NULL)+scale_x_continuous(breaks = NULL, limits = c(1,5))
   

```

### Variable Importance 

Una manera de interpretar los resultados es contar para cada variable cuántos splits de hicieron. Un valor grande es un indicativo de que esa variable es muy útil para predecir $y$. 

\begin{center}
\includegraphics[height = 5cm, width = 8cm]{var_imp_btc.png}
\end{center}

### Random Forests 

Un problema de los Bagging Trees es que incluso con el remuestreo, los árboles tienen mucha probabilidad de ser muy parecidos entre ellos. Muy **correlacionados**.

Un Random Forest resuelve esto así (para una base $(n,p)$: 

1. Construye $B$ muestras con reemplazo

**2. Toma una muestra de las columnas $m = \sqrt{p}$**

3. Construyes un árbol en cada muestra $B$: $\hat f^b(X)$ $b \in \{1,...,B\}$

4. Construye el Bag estimate: 

  - Para árboles de regresión: $\hat f_{bag}(X) = \frac{1}{B}\Sigma_{b=1}^B \hat f^b(X)$
  
  - Para árboles de clasificación: Majority vote. La clase más vota es a predicción.


### Random Forest vs Bagging Trees

\begin{center}
\includegraphics[width = 8cm]{OOB.png}
\end{center}


### Cuántas columnas $m$

\begin{center}
\includegraphics[width = 8cm]{m.png}
\end{center}


### Random Forest: Hiperparámetros

Noten como los hiperparámetros a estimar van aumentando:

- En regularización L1 o L2 (LASSO, Ridge) sólo teníamos a $\lambda$ estimable via CV

- En los árboles teníamos `min.obs` y `min.dev`. Como se estimaban?

En el Random Forest tengo:

- `min.obs` 

- `min.dev`

- `m`

- `B` o `num.trees`

Acá necesitamos estimar a prueba y error. Optimizamos el algoritmo como una optimización númerica.


### Random Forests en `R`

`library(ranger)` y `library(randomForest)` son las librerías más comunes en `R`. La primera es lo mismo que la segunda pero mucho más rápida en estimar.  
```{r eval=FALSE, echo=TRUE}
detectCores()
cl<-makeCluster(12)
cl

# Estimation 
a<-Sys.time()
rf<-ranger(y~., data = x, classification = T, num.trees = 750)

Sys.time() -a

save(rf, file = 'Modelos/rf_1000.Rdata')
stopCluster(cl)

```

### `ranger` prediction

```{r eval=FALSE, echo=TRUE}

# Out of Bag
oob<-rf$predictions

# prediction vectors 
pred_val<-predict(rf, data = validation)$predictions

```

### `Ranger`

\begin{center}
\includegraphics[width = 12cm]{ranger1.png}
\end{center}

### `Ranger`

\begin{center}
\includegraphics[width = 12cm]{ranger2.png}
\end{center}

### `Ranger`

\begin{center}
\includegraphics[width = 12cm]{ranger3.png}
\end{center}



### Ejemplo: California Housing data 

Tenemos una base de precios de casas en California que contiene, por código censal: 

- Precios medianos de las casas 

- Población e ingreso

- Características del hogar

Queremos predecir el precio para cada código censalL 1) LASOO 2) Prune Tree 3) Random Forest

```{r eval=FALSE, echo=TRUE}
carf<-ranger(logMedVal~., data= CAhousing,
             write.forest = T, num.trees = 200, 
             min.node.size = 25, 
             importance = 'impurity')
```

### Ejemplo: California Housing data 

\begin{center}
\includegraphics[width = 12cm]{ca_arbol.png}
\end{center}

### Fit Arbol

\begin{center}
\includegraphics[width = 12cm]{arbol_ca.png}
\end{center}

### Fit LASSO

\begin{center}
\includegraphics[width = 12cm]{Lasso_ca.png}
\end{center}

### Fit Random Forest 

\begin{center}
\includegraphics[width = 12cm]{rf_ca.png}
\end{center}

### Comparativa

\begin{center}
\includegraphics[width = 12cm]{comparativa.png}
\end{center}


### Boosting 

El Boosting es una de las ideas más poderosas para crear buenos modelos de predicción. Al igual que los random forests, los boosting machines combinan muchos "weak" learners que funcionan como un comité. 

Sin embargo, hay diferencias gigantes en cómo funciona el Boosting. Intuitivamente: 

- En el boosting, el cómite se agrupa de **super weak** learner (incluso stumps), mientras que RF agrupa árboles más grandes e independientes.

- En el boosting, los learners son construídos de manera secuencial; en el RF, de manera paralela.

- Al estimarse de manera secuencial, los Boosting algorithms **siempre** toman el último learner y lo usan para poner **enfoque** en lo que le falta aprender al algoritmo. Esto es como un proceso que va aprendiendo que es en lo que es malo automáticamente para corregirlo! 


### Boosting

- En cada nueva iteración, puede agregarse bagging para que el algoritmo observe nuevos puntos  

- Finalmente, muchos boosting algoritmos incluyen un **peso** o **learning rate** cuando agregan los weak learners. Esto funciona como un regulizador! 

En suma, un boosting algorithm estima mucho weak learners y los agrega. En cada learner, pone enfoque en los errores. Así, el algoritmo mejora donde más hace falta. Finalmente asigna un peso a cada learner en ese comité final. 


\begin{center}
\includegraphics[width = 8cm]{boosting.png}
\end{center}






### ADABoost 

Tomemos el ADABoost como ejemplo:

- Construye un weak learner $G_m(x)$, para $m \in \{ 1,...M \}$. En él, le asigna a cada punto $(y_i,X_i)$ un peso $w_i=\frac{1}{N}$ 

- Estima el error de ese weak learner 


$$err=\frac{\Sigma_{i=1}^Nw_iI(y_i \neq G_m(x_i))}{\Sigma_{i=1}^Nw_i}$$

- Le asigna un "peso" (votos) a $G_m(x)$ en el comité final $\alpha_m$ por $G_m$:

$$\alpha_m = log((1-err)/err)$$



### ADABoost

Noten como un weak learner muy bueno $err \rightarrow 0$, $\alpha_m \rightarrow \infty$. Por otro lado, si $err \rightarrow 1$, $\alpha_m \rightarrow 0$. 

- Modifica los pesos de las obsevaciones de acuerdo al acierto: a las observaciones que el modelo atinó, $\downarrow w_i$; para observaciones donde el modelo falló $\uparrow w_i$. 

$$w_i \leftarrow w_i *e^{\alpha_m*I(y_i \neq G_m(x_i))}$$

Noten como $\partial w_i/\partial[I(y_i \neq G_m(x_i))]>0$. Es decir, el peso sube si no le atine en esa observación. 

- Estima el siguiente $G_m(x)$ aplicando los nuevos pesos.

- Genera una estimación final con base en todos los learners: $G(x)=\Sigma_{m=1}^M\alpha_mG_m(x)$


### Gradient Boosting 

Vayamos al Gradient Boosting. Este difiere un poco del ADABoost en cómo se enfoca en los errores. Sin embargo, la idea es muy parecida: 

1 Empieza con un first learner

2 Para cada learner $M$: 

- Estima el siguiente learner usando los **errores** como variable objetivo. De aquí su nombre **gradient**

- Estima el nuevo learner $\gamma$ con base en las regiones marcadas por el nuevo learner

3 Actualiza el modelo agregado con el aprendizaje 




### Gradient Boosting: Loss function

Empieza encontrando una Loss function por tipo de problema que sea diferenciable:


\begin{center}
\includegraphics[width = 10cm]{loss_functions.png}
\end{center}


### Gradient Boosting: Algoritmo

1. Empiezas con un algoritmo y estimación base 

$$f_0(x_i)=argmin_\gamma \Sigma_{i=1}^NL(y_i, \gamma)$$

- Típicamente $\gamma$ será el promedio de las observaciones para regresión o la moda para clasificación. $f(.)$ normalmente será un árbol; pero puede ser otro estimador

2. Para cada secuencia $m=1,2, ..., M$

a) Para cada observación, estima el gradiente $r_i$ (los residuales!) usando el algoritmo de la iteración anterior $f_{m-1}$:

$$r_{im}=- \left[ \frac{\partial L(y_i,f(x_i))}{\partial f} \right]_{f=f_{m-1}}$$

### Gradient Boosting: Algoritmo


b) Entrena un árbol con los residuales $r_{im}$ como variable objetivo. Esto arroja las regiones $R_{jm}$

c) En cada región estima $\gamma_{jm}$: El promedio de las observaciones $y_i$ (o moda en clasificación)

d) Actualiza $f_m(x)=f_{m-1}(x)+\lambda \Sigma_{j=1}^{Jm}\gamma_{jm}I(x \in R_{jm})$


De vuelta: construyes un grupo de muchos modelos muy simples, haces que cada modelo tome los errores del anterior y se enfoque en eso. Finalmente, juntas todos los modelos, ponderándolos por un $\lambda$, que regulariza el aprendizaje.



### Gradient Boosting Machines


Parámetros a tunear: 

- Número de árboles $M$: A diferencia de bagging y RF, si $M$ es muy grande, caeremos en overfitting. Usamos cross-validation para obtener $B$

- $\lambda$ shrinkage parameter: Este controla (regulariza) al ritmo al que el algoritmo lee. Baja el peso de cada nuevo árbol. Se interpreta como "escepticismo". Típicamente tiene valores de 0.01, 0.001.

- $d$ Número de splits en cada árbol. Típicamente $d=1$

- $\gamma$ la función de aprendizaje

### Qué tan grandes deben ser los árboles?

\begin{center}
\includegraphics[width = 8cm]{d.png}
\end{center}


### Extreme Gradient Boosting Machines (XGBoosting)

Una generalización de los Gradient Boosting Machines; se agregan más hiperparámetros: 

- $M$ `nrounds`: Número de árboles

- `early_stopping_rounds`: Resuelve la miopía y la gobierna. Cuántas rondas pueden pasar sin que disminuya el RSS para parar. Igual los hace más ligeros que un Random Forest.

- $d$ `max_depth`: El tamaño máximo permitido por árbol.

- `min_child_weight`: Muy similar a mínimas observaciones en los nodos terminales del árbol. 

- `gamma`: La ganancia mínima requerida dentro de cada árbol. Controla la profundidad y cantidad de árboles.

- $\lambda$ `eta`: El tasa de aprendizaje $[0,1]$. 

- `subsample`: La muestra sobre N que toma para estimar el árbol

- `colsample_`: La muestra de columnas por cada árbol (Como en RF!)

- `lambda`: Regularización del meta proceso estilo LASSO 


### Extreme Gradient Boosting Machines (XGBoosting)

- Como ven, los XGB combinan muchos conceptos que vimos en trees, LASSO e incluso Random Forest. 

- Esto los convierte en los algoritmos favoritos (por poderosos) de muchos científicos de datos.

- Tienden a ser más ligeros que los Random Forests, ya que cada árbol es muy pequeño

- la librería para estimarlos en `R` es `library(xgboost)`

- Típicamente se tunea estos parámetros haciendo un grid de todos ellos (que cubra mucho del espacio) y despues se observa cuál combinación fue la ganadora.  Esta es la parte más complicada


### Ejemplo en `R`


```{r eval=FALSE, echo=TRUE}
library(tidyverse)
library(tidymodels) # caret igual sirve
library(xgboost)

# Loading the data lists for training 
load('Bases output/universo_training.Rdata')

# Creating the grid that covers must of the hyper space 
xgb_grid<-grid_latin_hypercube(
                tree_depth(), 
                min_n(), 
                loss_reduction(), 
                sample_size = sample_prop(), 
                learn_rate(),
                size = 15)

```



### Ejemplo en `R`


```{r eval=FALSE, echo=TRUE}
Xs<-sparse.model.matrix(~. + 0, data = data)
y<-data$y

# XGBOOST matrix 
training<-xgb.DMatrix(data = Xs, label = y)

# Watchlist 
watchlist<-list(training = training, validation = validation)
names(watchlist$validation)<- names(watchlist$training)

# transforming xgb_grid to have proper names 
xgb_grid<-xgb_grid %>% 
                rename(max_depth = tree_depth, 
                       min_child_weight = min_n, 
                       gamma = loss_reduction, 
                       subsample = sample_size, 
                       eta = learn_rate) %>% 
                mutate(eval_metric = 'auc') 

```



### Ejemplo en `R`

```{r eval=FALSE, echo=TRUE}

a<-Sys.time()
xgb_eth<-map(xgb_grid_list, 
             function(x)
               xgb.train(params = as.list(x), 
                         data = training$buy_eth, 
                         nrounds = 300, 
                         objective = "binary:logistic", 
                         early_stopping_rounds = 5, 
                         verbose = 1, 
                         watchlist = c(training = watchlist$training$buy_eth, 
                                       test = watchlist$validation$buy_eth)))
Sys.time() -a
save(xgb_eth, xgb_grid_list, training, watchlist, file = "Modelos/xgb_eth.Rdata")

```




### Ejemplo en `R`

```{r eval=F, echo=T}

########################################
# Choosing the best model for each coin
########################################
write.xlsx(xgb_grid, file = 'Tablas/xgb_grid.xlsx')

# Exporting the parameters and AUC 

# ETH 
params_eth<-map_dfr(xgb_eth, ~.$params)
params_eth<-bind_cols(params_eth, auc = map_dbl(xgb_eth, function(x) x$best_score))  
params_eth$model_number<-seq(1:15)


```

### Ejemplo en `R`

\begin{center}
\includegraphics[width = 10cm]{xgb_tuning.png}
\end{center}

