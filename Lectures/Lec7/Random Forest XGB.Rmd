---
title: 'Lec7: Random Forest y XGB'
author: "Isidoro Garcia Urquieta"
date: "2021"
output: beamer_presentation
slide_level: 3
fontsize: 10pt
header-includes: 
   \usepackage{graphicx}
   \usepackage{fancyhdr}
   \pagestyle{fancy}
   \setlength\headheight{3pt}
   \fancyhead[R]{\includegraphics[width=2cm]{logo-ITAM.png}}
   \lhead{\fontsize{8pt}{10pt}\selectfont \textit{Economía Computacional}}
   \definecolor{verdeitam}{RGB}{0,90,0}
   \setbeamercolor{structure}{fg=verdeitam}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

### Agenda

- Mini Repaso Trees 

- Bootrapping

- Bagging

- Random Forests 

- Boosting 

- Gradient Boosting Machines




### Resumen Trees

- Los árboles de decisión (Regresión y Clasificación) son algoritmos no paramétricos

- Son fáciles de interpretar! 

- Detectan no linealidades de manera automática

- Se estiman de manera **greedy**, donde se buscan splits que maximicen la varianza entre nodos (inter-varianza) y se minimice la varianze dentro de cada nodo (intra-varianza)

- Hay dos maneras de parar la estimación de los arboles: observaciones mínimas en los nodos terminales o disminución en el deviance mínima. 

- El greediness es miope. Esto es, un split malo puede seguirle un gran split. 

- Es díficil evitar el overfitting en los árboles. 

### Cost complexity pruning 

Cost complexity pruning (Weakest link pruning) es la manera más común de reducir el número de modelos candidatos y aplicar k-fold CV. 

El proceso funciona así: 

1. Estima árbol de manera greedy hasta llegar a `min obs`. 

2. Aplica un parámetro de complejidad $\alpha$ que castigue la complejidad del árbol. Esto te dará una secuencia de subárboles como función de $\alpha$.

3. Aplica k-fold CV para escoger $\alpha$ 

### Cost complexity pruning 


Veamos la formula de coxt complexity pruning:

$$\Sigma_{j=1}^J\Sigma_{R_j}(y_i - \hat y_i)^2 + \alpha|T|$$
Donde $T$ es el número de nodos terminales. Se ve familiar? 

Resulta que si un $\alpha$ muy grande, tendremos un árbol con $T_0 \subset T$. Mientras $\alpha$ tiende a cero $T_0 \rightarrow T$. 

$\alpha$ nos ayuda a tener un set de candidatos de modelo estimable vía K-fold CV!!


### Grafica de desempeño de CV tree

Noten como el CV es un buen estimador del desempeño fuera de la muestra (3 nodos terminales). Adicionalmente, noten como en el training set, el árbol siempre parece que va a estimar mejor mientras más grande haga los árboles.

\begin{center}
\includegraphics[width = 8cm]{cv_tree.png}
\end{center}


### Resumen Tree

- Para ayudar al overfitting, podemos hacer Tree pruning con Cross Validation 

- Esto hara que estimemos greedy y luego tengamos un set de árboles candidatos dependiendo de cuánto castigamos la complejidad de los mismos.

- Los prune.trees son más poderosos (predicción) que los árboles normales. Por ende, preferibles. 

- Aún así, los árboles son estimadores de alta varianza. Esto es, varían mucho dependiendo de la muestra en la que nos encontremos (no paramétrico). Por ende, necesitamos alguna manera de estabilizar las predicciones de los árboles para lograr mejor poder predictivo OOS. 


### Boostrapping 

Llegamos al segundo método de **remuestreo** (El primero fue CV)! 

El boostrapping es muy útil para conseguir errores estandares e intervalos de confianza cuando la distribución de algún estimador es desconocida, tenemos pocas observaciones o es muy díficil de estimar. 

Por ejemplo, en el caso de la econometría, podemos inferir la distribución de nuestro estimador $\hat \beta$ al remuestrear la base **con reemplazo** y estimar $\beta$ con cada muestra.  

Lo bonito del Boostrapping es que tiene otras aplicaciones además de la generación de errores estandár. Se puede aplicar a mejorar el poder predictivo de nuestros modelos estadísticos.

### Boostrapping 

Pasos: 

Dada una base de datos $\{ z_i\}_{i=1}^n$, para $b \in \{1,2,3...,B \}$ muestras: 

1. Tomas una muestra **con reemplazo** $\{ z_i^b\}_{i=1}^n$

2. Estimas $\beta_b$ usando la base de (1)

Luego, $\{ \beta_b\}_{b=1}^B$ es una aproximación de la distribución de $\hat\beta$

El vector de tamaño $B$ $\{ \beta_b\}_{b=1}^B$ puede ser usado como la distribución teorica que aprendimos en la clase 2 (inferencia!) que usaba el Teorema del Límite Central. 

$$\beta \in \hat \beta \pm 2sd(\hat \beta_b) $$
$$se(\hat \beta) \simeq sd(\beta_b)=\sqrt{\frac{1}{B}\Sigma_b(\hat \beta_b - \hat \beta)}$$
Donde $\hat \beta$ es el estimador original usando toda la base de datos. 


### Bagging 

Los árboles de decisión sufren alta varianza. Esto es, cuando dividimos la muestra en entrenamiento y validación y entrenamos el árbol (c/ CV pruning), los resultados pueden ser muy distintos en cada base. 

En el caso del LASSO, esto no era así. Porqué? 

  - Por que teníamos **estimadores** paramétricos que se estabilizan con cierta facilidad ($n>p$). 
  
El **Boostrapping Aggregation (Bagging!)** es un procedimiento muy utilizado para estabilizar la varianza en aprendizaje estadístico. 


### Bagging 

El Bagging utiliza el poder de las medias para crear estimadores estables. 

Recuerden que si tenemos un set de $n$ observaciones independientes $Z_1,Z_2,...,Z_n$ y *tenemos un procedimiento insesgado*: 

Promediamos las $Z_i$: $\bar Z$

Sabemos que:

$$\bar{Z}_n \rightarrow^DN(\mu_Z,\sqrt{\sigma^2/n})$$

En otras palabras, la varianza original de los datos $\sigma^2$ se reduce cuando sacamos promedios a $\sigma^2/n$! 


### Bagging en Árboles de Decisión

Pasos para aplicar Bagging en árboles de decisión: 

1. Construye $B$ muestras con reemplazo

2. Construyes un árbol en cada muestra $B$: $\hat f^b(X)$ $b \in \{1,...,B\}$

3. Construye el Bag estimate: 

  - Para árboles de regresión: $\hat f_{bag}(X) = \frac{1}{B}\Sigma_{b=1}^B \hat f^b(X)$
  
  - Para árboles de clasificación: Majority vote. La clase más vota es a predicción.

Queda pendiente algo: Cuántas submuestras $B$ tomar? Con $B$ alto caemos en overfitting?

### Bagging

Cuantas $B$ tomar? 

- En la práctica, necesitamos $B$ suficientemente grandes para estabilizar la varianza. Esto

- Otro punto importante es que un número grande $B$ no induce overfitting, al contrario. 

- Con: Estimar los árboles con demasiadas submuestras puede ser computacionalmente complejo. No obstante es alcanzable en una buena computadora (Parallel Computing!)

### Out-of-Bag

Resulta que, como en Cross-Validation, hay manera de estimar el error OOS desde la muestra de entrenamiento. 

Recuerden que en cada muestra $B$, dejamos algunas observaciones sin tomar. Estas observaciones pueden servir como estimador del error. Su nombre es out of bag. 

Los resultados muestran que en la práctica, los bagging trees superan en poder de predicción a los trees y pruned trees. 


Terminamos? que cosas se les ocurren que pueden seguir pasando con los Bagging Estimators?

### Interpretabilidad 

El bagging nos permite estabilizar las predicciones de los árboles y ganar poder predictivo. 

El costo asociado es la **interpretabilidad**

```{r echo=FALSE, fig.height=4.5, message=FALSE, warning=FALSE}
library(tidyverse)
options(scipen = 99)
grafica2<-
   tibble(flexibilidad = seq(1, 4, 1), 
          interpretabilidad = seq(4,1,-1), 
          metodo = c('LASSO', 'Regresión', 'Árboles de decisión', 'Bagging, Boosting'))

ggplot(grafica2, aes(flexibilidad, interpretabilidad, label = metodo))+geom_text()+
   theme_classic()+scale_y_continuous(breaks = NULL)+scale_x_continuous(breaks = NULL, limits = c(1,5))
   

```

### Variable Importance 

Una manera de interpretar los resultados es contar para cada variable cuántos splits de hicieron. Un valor grande es un indicativo de que esa variable es muy útil para predecir $y$. 

\begin{center}
\includegraphics[height = 5cm, width = 8cm]{var_imp_btc.png}
\end{center}

### Random Forests 

Un problema de los Bagging Trees es que incluso con el remuestreo, los árboles tienen mucha probabilidad de ser muy parecidos entre ellos. Muy **correlacionados**.

Un Random Forest resuelve esto así (para una base $(n,p)$: 

1. Construye $B$ muestras con reemplazo

**2. Toma una muestra de las columnas $m = \sqrt{p}$**

3. Construyes un árbol en cada muestra $B$: $\hat f^b(X)$ $b \in \{1,...,B\}$

4. Construye el Bag estimate: 

  - Para árboles de regresión: $\hat f_{bag}(X) = \frac{1}{B}\Sigma_{b=1}^B \hat f^b(X)$
  
  - Para árboles de clasificación: Majority vote. La clase más vota es a predicción.


### Random Forest vs Bagging Trees

\begin{center}
\includegraphics[width = 8cm]{OOB.png}
\end{center}


### Cuántas columnas $m$

\begin{center}
\includegraphics[width = 8cm]{m.png}
\end{center}

### Random Forests en `R`

`library(ranger)` y `library(randomForest)` son las librerías más comunes en `R`. La primera es lo mismo que la segunda pero mucho más rápida en estimar.  
```{r eval=FALSE, echo=TRUE}
detectCores()
cl<-makeCluster(12)
cl

# Estimation 
a<-Sys.time()
rf<-ranger(y~., data = x, classification = T, num.trees = 750)

Sys.time() -a

save(rf, file = 'Modelos/rf_1000.Rdata')
stopCluster(cl)

```

### `ranger` prediction

```{r eval=FALSE, echo=TRUE}

# Out of Bag
oob<-rf$predictions

# prediction vectors 
pred_val<-predict(rf, data = validation)$predictions

```

### `Ranger`

\begin{center}
\includegraphics[width = 12cm]{ranger1.png}
\end{center}

### `Ranger`

\begin{center}
\includegraphics[width = 12cm]{ranger2.png}
\end{center}

### `Ranger`

\begin{center}
\includegraphics[width = 12cm]{ranger3.png}
\end{center}



### Ejemplo: California Housing data 

Tenemos una base de precios de casas en California que contiene, por código censal: 

- Precios medianos de las casas 

- Población e ingreso

- Características del hogar

Queremos predecir el precio para cada código censalL 1) LASOO 2) Prune Tree 3) Random Forest

```{r eval=FALSE, echo=TRUE}
carf<-ranger(logMedVal~., data= CAhousing,
             write.forest = T, num.trees = 200, 
             min.node.size = 25, 
             importance = 'impurity')
```

### Ejemplo: California Housing data 

\begin{center}
\includegraphics[width = 12cm]{ca_arbol.png}
\end{center}

### Fit Arbol

\begin{center}
\includegraphics[width = 12cm]{arbol_ca.png}
\end{center}

### Fit LASSO

\begin{center}
\includegraphics[width = 12cm]{Lasso_ca.png}
\end{center}

### Fit Random Forest 

\begin{center}
\includegraphics[width = 12cm]{rf_ca.png}
\end{center}

### Comparativa

\begin{center}
\includegraphics[width = 12cm]{comparativa.png}
\end{center}

### Boosting 

Ahora discutamos Boosting. Como el bagging y el CV, se puede utilizar en muchos contextos. 

En Bagging creamos varias bases de datos, estimamos árboles en cada uno y promediamos. En Boosting, los árboles crecen **secuencialmente**. 

- Esto es, cada árbol usa información del árbol anterior. 

Pasos. Para $b \in \{1,...,B \}$: 

1. Estimas el primer árbol $\hat f^b(X)$ sobre los residuales $r$ con $d$ splits ($d+1$ nodos terminales)

2. Obtienes una nueva función de prediccion: $\hat f(x) \leftarrow \hat f(x) + \lambda \hat f^b(X)$

3. Actualiza los residuales: $r_i \leftarrow r_i + \lambda \hat f^b(X)$

El modelo final: $\hat f(x) = \Sigma_{b=1}^B \lambda \hat f^b(x)$

### Gradient Boosting Machines

Cuál es la intuición del Boosting?

- En lugar de tener una combinación de arboles que aprenden mucho tengamos árboles sencillos

- Cada árbol va a estar aprendiendo lo siguiente más importante de $f(x)$. Se toma el árbol que disminuye más el RSS (gradient)

- Esto hace que las GBM aprendan **lento**.

Parámetros a tunear: 

- Número de árboles $B$: A diferencia de bagging y RF, si $B$ es muy grande, caeremos en overfitting. Usamos cross-validation para obtener $B$

- $\lambda$ shrinkage parameter: Este controla (regulariza) al ritmo al que el algoritmo lee. Baja el peso de cada nuevo árbol. Se interpreta como "escepticismo". Típicamente tiene valores de 0.01, 0.001.

- $d$ Número de splits en cada árbol. Típicamente $d=1$

### Extreme Gradient Boosting Machines (XGBoosting)

Una generalización de los Gradient Boosting Machines. Igual busca disminuir el gradiente de RSS mediante Boosting. La diferencia es que los XGBoosting tienen muchos más parámetros a tunear (aquí algunos):

- $B$ `nrounds`: Número de árboles

- `early_stopping_rounds`: Resuelve la miopía y la gobierna. Cuántas rondas pueden pasar sin que disminuya el RSS para parar. Igual los hace más ligeros que un Random Forest.

- $d$ `max_depth`: El tamaño máximo permitido por árbol.

- `min_child_weight`: Muy similar a mínimas observaciones en los nodos terminales del árbol. 

- `gamma`: La ganancia mínima requerida dentro de cada árbol. Controla la profundidad y cantidad de árboles.

- $\lambda$ `eta`: El tasa de aprendizaje $[0,1]$. 

- `subsample`: La muestra que toma para estimar el árbol ($m$ en RF) 

### Extreme Gradient Boosting Machines (XGBoosting)

- Como ven, los XGB combinan muchos conceptos que vimos en trees, LASSO e incluso Random Forest. 

- Esto los convierte en los algoritmos favoritos (por poderosos) de muchos científicos de datos.

- Tienden a ser más ligeros que los Random Forests. 

- la librería para estimarlos en `R` es `library(xgboost)`

- Típicamente se tunea estos parámetros haciendo un grid de todos ellos (que cubra mucho del espacio) y despues se observa cuál combinación fue la ganadora. 




