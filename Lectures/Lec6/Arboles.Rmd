---
title: 'Lec5: Árboles de Decisión'
author: "Isidoro Garcia Urquieta"
date: "2023"
output: beamer_presentation
slide_level: 3
fontsize: 10pt
header-includes: 
   \usepackage{graphicx}
   \usepackage{fancyhdr}
   \pagestyle{fancy}
   \setlength\headheight{3pt}
   \fancyhead[R]{\includegraphics[width=2cm]{logo-ITAM.png}}
   \lhead{\fontsize{8pt}{10pt}\selectfont \textit{Economía Computacional}}
   \definecolor{verdeitam}{RGB}{0,90,0}
   \setbeamercolor{structure}{fg=verdeitam}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

### Agenda

- Árboles de Clasificación 

- Árboles de Regresión

- Recursive binary splitting: Decision split for each case

- Tree Pruning 

- Linear vs Trees 

- Boostrapping & Bagging 


### Qué es un arbol de decisión

Los árboles de decisión son sistemas lógicos que mapean características $X_i$ a predicciones $Y_i$. 

- Esto es una secuencia de decisiones binarias para llegar a una conclusión. 

- Son jerarquicos. Siempre empiezan por el mismo lado. 

- La predicción final es un nodo terminal


### Ejemplo 

\begin{center}
\includegraphics[width = 10cm]{arbol.png}
\end{center}

### Elementos de un Árbol de decisión

Los árboles se componen de: 

 - Un nodo inicial/raíz
 
 - Nodos internos, cuya separación se marca por alguna $X_i$
 
 - Nodos terminales/ Leafs
 
Las predicciones se marcan por algún estadístico (i.e. la media) de las observaciones de algún nodo terminal. 

### Ejemplo II

\begin{center}
\includegraphics[width = 11cm]{arbol2.png}
\end{center}

### Arbol graficamente

En realidad, los árboles son un método de predicción **no paramétrico**. La predicción sucede mediante la estratificación del espacio de covariables 

\begin{center}
\includegraphics[width = 11cm]{arbol3.png}
\end{center}


### Árboles como una piecewise function

En los árboles de regresión, $y_i$ es continua. La gran mayoría de las veces, $\hat{y_{iR}}$ va a ser la media de $y_i$ en el nodo terminal $R$. 

Veamos como los árboles de regresión son como modelos de regresión piece-wise: 

$$D_{iR}= 1 \{i \in R \}$$

Se sigue que: 

$$\hat{f(X)} = \hat{y_i}=\Sigma_{r=1}^R\hat\beta_RD_{iR}+\epsilon_i$$
Donde $\hat{\beta_R}$ varía dependiendo de si es un árbol de regresión o clasificación.



### Flexibilidad funcional

Noten como los árboles detectan las no-linealidades (interacciones en los modelos de regresión) de $f(.)$ de manera automática. Incluso algunas de alto valor polinómico. 

\begin{center}
\includegraphics[width = 8cm]{regression_tree.png}
\end{center}


### Funciones objetivo para árboles

Como siempre, queremos disminuir el error de predicción fuera de la muestra. En el caso de los árboles, estas son las funciones a minimizar: 

Para $R$ particiones del espacio de covariables. Donde hay $j$ particiones.

- **Regresión**: $RSS = \Sigma_{j=1}^J\Sigma_{R_j}(y_i - \hat y_i)^2$. Esta es la misma desviación promedio que hemos visto para los modelos lineales. 

- **Clasificación**: Para $k$ clases

  - Entropía: $-\Sigma\hat p_{k}log(\hat p_{k})$
 
  - Gini: $\Sigma_k=\hat p_k(1-\hat p_k)$

Noten como para ambas medidas estamos buscando **pureza**. Cuando $\hat p_k$ es cero o uno, Gini y Entropía se hacen más pequeños. En estos score predecidos, significaría que la mayoría de las observaciones en el *leaf* son de una sola clase. 




### Cómo estimo los árboles? 

Cómo hacer las particiones? Otra vez estamos enfrentados al problema de dimensionalidad. Es casi imposible estimar cada árbol posible. 

- Cómo escoger que splits? Para cada $X$, el árbol puede evaluar un corte en cada punto del dominio. Son muchas posibilidades! 

- Para una profundidad $d$, un årbol puede tener $2^d$ nodos terminales (asumiendo un árbol simétrico)

Si en cada nodo evalúa cada variable, tenemos $A_p*p*2^d$ posibilidades (Donde A es la cardinalidad del dominio de cada variable). 

Este número es gigante incluso para una profundida y base pequeña!! 



### Cómo estimo los árboles? 

Los árboles típicamente se estiman **top-down** y de forma **greedy**. Esto se llama **CART: Classfication and Regression Tree** algorithm de Brieman (1984): 

1. Empiezas en el nodo inicial

2. Escoges el punto de corte & $x_i$ que disminuyen más el error de predicción

3. Divides la muestra en los nodos determinados en (2). 

4. Repites 2 y 3 iterativamente 


### Cómo estimo los árboles? 

Veamos que significa cada parte: 

**Top-Down**

- Son modelos jerárquicos. Una vez hecho un split, este no se puede deshacer. 

- Los primeros splits juegan un rol super importante. 

**Greedy**

- En cada splt, el árbol busca el mejor splt posible para disminuir el error de predicción. Ojo! No busca la secuencia de split; el mejor split en el margen. Por eso se les denomina greedy. 

### Hasta donde estimar el árbol

El algoritmo descrito en las láminas anteriores tiene mucha propensión a caer en overfitting. Por ende, necesita reglas para detener el crecimiento del árbol. 

Las dos más comunes son: 

- Min final node `minsize`: Estima el árbol hasta que haya z o más observaciones en cada nodo terminal (típicamente 10)

- Min node size/ `min cut`: Cada split debe dejar al menos `min cut` observaciones de cada lado  (típicamente 5)

- Minimum loss improvement / `min dev`: Estima el árbol hasta que la disminución de cada split llegue a un corte de disminución. 

Ambas, especialmente la tercera estrategia tienen un problema de miopía: Un split mediocre puede ser el preludio de un split mucho mejor. 


### Árboles de Regresión

Los árboles de regresión pueden ser entendidos como una función piecewise de las variables  $X$

1. $y_i$ es continua

2. La decisión de split es la disminución del Residual Sum of Squares (RSS)

3. Las predicciones finales son medias $\frac{1}{N_r}\Sigma_{i \in r}y_{i}$


### Estimación de árboles regresión

Esto es, para una $X_k$ y un punto de corte $s$ definimos dos espacios $R_l$ y $R_r$: 

$$R_l(k,s): \{i:x_{ik}<s\} \ \ \ \ R_r(k,s): \{i:x_{ik}\geq s\}$$

El RSS resultante es ahora: 

$$RSS^1 = \Sigma_{i \in R_l}(y_i - \hat y_i)^2+\Sigma_{i \in R_r}(y_i - \hat y_i)^2$$

Donde $RSS^1<RSS (modelo \ anterior)$

El proceso sigue hasta que se llega a un criterio de finalización. Noten como para una variable $X \in \{1,...,100\}$ el árbol puede evaluar cortes en 1,2,3,4..... en cada valor! 





### Árboles en R 

Las librerías para estimar árboles es `tree` y `rpart`. La primera es la más sencilla; la segunda produce årboles más bonitos de graficar. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(MASS)
library(tidyverse)
library(tree)
library(rpart)
library(broom)
library(rpart.plot)
data('mpg', package='ggplot2')
mpg <- mpg %>%
  mutate(drv=factor(drv), 
         cyl = factor(cyl) )
```


### Árboles en R 

Una estimación básica

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.width=7, fig.height=3}
t <- rpart( cty ~ displ + year + cyl + drv, data=mpg)
rpart.plot(t, digits =3)  
```

### Árboles de Regresión en R 

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(t)
```


### Movemos `tree::min.dev` `rpart::cp`

```{r echo=TRUE, message=FALSE, warning=FALSE, , fig.width=7, fig.height=4}
t2 <- rpart( cty ~ displ + year + cyl + drv, data=mpg, 
             cp = 0.0001)
rpart.plot(t2, digits =3)  

```

### Movemos `rpart::minbucket`

```{r echo=TRUE, message=FALSE, warning=FALSE, , fig.width=7, fig.height=4}
t3 <- rpart( cty ~ displ + year + cyl + drv, data=mpg, 
             minbucket = 50)
rpart.plot(t3, digits =3)  

```


### Árboles de Clasificación

Los árboles de clasificación son muy similares a los árboles de regresión. Las diferencias son: 

1. $y_i \in \{ 1,2,3,...K\}$

2. La decisión de split es Gini o Entropy (vs RSS)

3. Las predicciones NO son medias. 

Como son las predicciones entonces? Los árboles de clasificación te arrojan dos tipos: 

a. Predicción categórica: El árbol predice la clase $k$ más común en $r \in R$

b. Predicción continua: El árbol te da las probabilidades o scores como la proporción de casos en $r \in R$


### Estimación de árboles de clasificación

Buscamos minimizar el error de clasificación de entropía $-\Sigma\hat p_{k}log(\hat p_{k})$ o Gini $\Sigma_k=\hat p_k(1-\hat p_k)$


Esto es, para una $X_k$ y un punto de corte $s$ definimos dos espacios $R_l$ y $R_r$:

$$R_l(k,s): \{i:x_{ik}<s\} \ \ \ \ R_r(k,s): \{i:x_{ik}\geq s\}$$

El estadístico resultante es ahora: 

$$Gini^1 = \Sigma_{i \in R_l}\Sigma_k\hat p_k(1-\hat p_k)+\Sigma_{i \in R_r}\Sigma_k\hat p_k(1-\hat p_k)$$

Donde $Gini^1<Gini (modelo \ sin \ split)$



### Ejemplo Clasificación

```{r echo=TRUE, message=FALSE, warning=FALSE}
t3 <- rpart( class ~ displ + year + cyl + drv, data=mpg, 
             cp=0.05)
rpart.plot(t3, digits =2,type = 0)  

```

### Ejemplo Clasificación

```{r echo=TRUE, message=FALSE, warning=FALSE}
predict(t3)[1:5, ]
```




### Tree pruning 

Hasta ahora vimos que estimamos el árbol hasta que `mincut` o `min dev` lo marquen. Esto genera tres problemas: Es díficil no caer en overfitting y no consideramos la posibilidad de split mediocres seguidos de grandes splits; Hay muchas posibilidades!

Por lo anterior, es mejor estrategia estimar un árbol grande y despuer podarlo (prune!). Como? .... Cross-Validation! 

- Estimemos varios árboles y, usando validación cruzada (k-fold) escojamos el mejor modelo. 

Preguntas: 

- Por qué no podemos usar criterios de información aquí como AICc? 

- Por qué no tenemos un buen estimado de los grados de libertad (No paramétrico!)

- Cuales son las posibilidades de árboles a estimar? 

- Aquí es más díficil reducir el problema de **modelos candidatos** vs LASSO. 


### Cost complexity pruning 

Cost complexity pruning (Weakest link pruning) es la manera más común de reducir el número de modelos candidatos y aplicar k-fold CV. 

El proceso funciona así: 

1. Estima árbol de manera greedy hasta llegar a `min obs`. 

2. Aplica un parámetro de complejidad $\alpha$ que castigue la complejidad del árbol. Esto te dará una secuencia de subárboles como función de $\alpha$.

3. Aplica k-fold CV para escoger $\alpha$ 


### Cost complexity pruning 

Veamos la formula de coxt complexity pruning:

$$\Sigma_{j=1}^J\Sigma_{R_j}(y_i - \hat y_i)^2 + \alpha|T|$$
Donde $T$ es el número de nodos terminales. Se ve familiar? 

Resulta que si un $\alpha$ muy grande, tendremos un árbol con $T_0 \subset T$. Mientras $\alpha$ tiende a cero $T_0 \rightarrow T$. 

$\alpha$ nos ayuda a tener un set de candidatos de modelo estimable vía K-fold CV!!


### Grafica de desempeño de CV tree

Noten como el CV es un buen estimador del desempeño fuera de la muestra (3 nodos terminales). Adicionalmente, noten como en el training set, el árbol siempre parece que va a estimar mejor mientras más grande haga los árboles.

\begin{center}
\includegraphics[width = 8cm]{cv_tree.png}
\end{center}



### Ejemplo de CV tree 

```{r echo=TRUE, message=FALSE, warning=FALSE}
cpus.ltr2 <- tree(log10(perf) ~ syct+mmin+mmax+cach
                 +chmin+chmax, cpus, minsize = 1)
(cv_tree<-cv.tree(cpus.ltr2, K= 5))

```

### Ejemplo de CV tree 

El resultado `size` es el numero de nodos terminales; `dev` corresponde al deviance CV; `k` = $\alpha$. 

Vemos que con 10 nodos terminales, logramos la menor deviance.

### Ejemplo de CV tree 

```{r echo=FALSE, fig.width=7, fig.height=4 , message=FALSE, warning=FALSE}
base_grafica<-
   tibble(size = cv_tree$size, 
          deviance = cv_tree$dev, 
          alpha = cv_tree$k)

ggplot(base_grafica, aes(size, deviance))+
   geom_line()+geom_point()+theme_bw()

```

### Ejemplo de CV tree 

```{r echo=FALSE, fig.width=7, fig.height=4 , message=FALSE, warning=FALSE}
ggplot(base_grafica, aes(alpha, deviance))+
   geom_line()+geom_point()+theme_bw()

```

### Árboles vs métodos lineales 

Qué es mejor? Métodos lineales o árboles? Depende del problema. 

Si el problema tiene altas no-linealidades, los árboles son mucho mejores. Esto es porque detectan las mismas de manera automática. 

Si el problema es más aproximable de manera lineal, los algoritmos lineales lo harán mejor. 

Pros árboles: 

- Muy intuitivos. Esto los hace muy fáciles de explicar. 

- Parecen ser más intuitivos porque se asemejan más al razonamiento humano (ifelse nodes). 

- Los árboles pueden manejar fácilmente variables categóricas sin necesidad de crear dummys. 

### Árboles vs métodos lineales 

Cons: 

- Al ser un método no paramétrico. Tienden a no ser estimadores muy estables antes nuevos datos. 

- Incluso con CV, es díficil no caer en overfitting. 


### Árboles vs métodos lineales 


\begin{center}
\includegraphics[width = 8cm]{lineal_no_linear.png}
\end{center}




### Ejemplo II 

```{r echo=TRUE, message=FALSE, warning=FALSE, , fig.width=7, fig.height=4}
t3 <- cv.tree( tree(cty ~ displ + year + cyl + drv, data=mpg), K = 5)
t3
```


### Ejemplo II

```{r echo=FALSE, fig.width=7, fig.height=4 , message=FALSE, warning=FALSE}
base_grafica<-
   tibble(size = t3$size, 
          deviance = t3$dev, 
          alpha = t3$k)

ggplot(base_grafica, aes(size, deviance))+
   geom_line()+geom_point()+theme_bw()

```


### Ejemplo II 

```{r echo=FALSE, fig.width=7, fig.height=4 , message=FALSE, warning=FALSE}
ggplot(base_grafica, aes(size, alpha))+
   geom_line()+geom_point()+theme_bw()

```



### Prunes tree

Escogemos el 

```{r echo=TRUE, message=FALSE, warning=FALSE, , fig.width=7, fig.height=4}
t4 <- prune(t, cp = 0.05)
rpart.plot(t4, digits =3)  

```





### Resumen Trees

- Los árboles de decisión (Regresión y Clasificación) son algoritmos no paramétricos

- Son fáciles de interpretar! 

- Detectan no linealidades de manera automática

- Se estiman de manera **greedy**, donde se buscan splits que maximicen la varianza entre nodos (inter-varianza) y se minimice la varianze dentro de cada nodo (intra-varianza)

- Hay dos maneras de parar la estimación de los arboles: observaciones mínimas en los nodos terminales o disminución en el deviance mínima. 

- El greediness es miope. Esto es, un split malo puede seguirle un gran split. 

- Es díficil evitar el overfitting en los árboles. 

### Resumen Tree

- Para ayudar al overfitting, podemos hacer Tree pruning con Cross Validation 

- Esto hara que estimemos greedy y luego tengamos un set de árboles candidatos dependiendo de cuánto castigamos la complejidad de los mismos.

- Los prune.trees son más poderosos (predicción) que los árboles normales. Por ende, preferibles. 

- Aún así, los árboles son estimadores de alta varianza. Esto es, varían mucho dependiendo de la muestra en la que nos encontremos (no paramétrico). Por ende, necesitamos alguna manera de estabilizar las predicciones de los árboles para lograr mejor poder predictivo OOS. 

- Los Random Forests y los Gradient Boosting Machines usan Boostrapping y otros métodos para lograr esto! (Próxima clase!)



### Bias and variance ejemplo

\begin{center}
\includegraphics[width = 8cm]{bias_variance.png}
\end{center}

